from xgboost import XGBRegressor as XGBR
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestRegressor as RFR
from sklearn.linear_model import LinearRegression as LinearR
from sklearn.model_selection import KFold, cross_val_score as CVS, train_test_split as TTS
from sklearn.metrics import mean_squared_error as MSE
import pandas as pd
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
from time import time
import datetime
import numpy as np
import pickle
import xgboost as xgb
import matplotlib.pyplot as plt
from xgboost import XGBClassifier as XGBC
from sklearn.metrics import confusion_matrix as cm, recall_score as recall,roc_auc_score as auc
from sklearn.metrics import auc

#import the dataset
df = pd.read_csv("model_build.csv")
df.set_index(df.Seq,inplace=True,drop=True)
df.drop(columns="Seq",axis=1,inplace=True)
x = df.iloc[:,0:-1]
y = df.iloc[:,-1]
dfull = xgb.DMatrix(x,y)

#define the function to draw the learning curve
def plot_learning_curve(estimator,title,X,y,ax=None,ylim=None,cv=None,n_jobs=None):
    
    from sklearn.model_selection import learning_curve
    import matplotlib.pyplot as plt
    import numpy as np
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y,shuffle=True,cv=cv,random_state=420,n_jobs=n_jobs)      
    if ax == None:
        ax = plt.gca()
    else:
        ax = plt.figure()
    ax.set_title(title)
    if ylim is not None:
        ax.set_ylim(*ylim)
    ax.set_xlabel("Training examples")
    ax.set_ylabel("Score")
    ax.grid()
    ax.plot(train_sizes, np.mean(train_scores, axis=1), 'o-'
            , color="r",label="Training score")
    ax.plot(train_sizes, np.mean(test_scores, axis=1), 'o-'
            , color="g",label="Test score")
    ax.legend(loc="best")
    return ax

#draw the learing curve,the first parameter is the default parameter and the second parameted is the optimized parameter
param1 = {'verbosity':0,'objevtive':'binary:logistic',"eta":1}
param2 = {'verbosity':0,'objective':'binary:logistic',"eta":0.1,"max_depth":2,"gamma":3,"colsample_bytree":0.2,"alpha":0.2}
num_round = 200

cvresult1 = xgb.cv(param1, dfull, num_round,nfold=5,metrics=("error"))
cvresult2 = xgb.cv(param2, dfull, num_round,metrics=("error")) 
plt.figure(figsize=(8,3),dpi = 300)
plt.grid()

plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)

plt.plot(range(1,201),cvresult1.iloc[:,0],c="y",label="default train")
plt.plot(range(1,201),cvresult1.iloc[:,2],c="red",label="default test")

plt.plot(range(1,201),cvresult2.iloc[:,0],c="blue",label="optimized train")
plt.plot(range(1,201),cvresult2.iloc[:,2],c="green",label="optimized test")

plt.ylabel("error",fontsize = 18.5)
plt.legend(loc="lower right",fontsize = 9)
plt.show()

#Draw the roc curve
xtrain,xtest,ytrain,ytest = TTS(x,y,test_size=0.3,random_state=40)
dtrain = xgb.DMatrix(xtrain,ytrain)
bst = xgb.train(param2,dtrain,num_round)
dtest = xgb.DMatrix(xtest,ytest)
test_score = bst.predict(dtest)

font1 = {'family' : 'Arial','weight' : 'bold','size' : 22.5}
font2 = {'family' : 'Arial','weight' : 'bold','size' : 25}
font3 = {'family' : 'Arial','weight' : 'bold','size' : 20}

mpl.rc('axes', lw=1.5)
fpr,tpr,threshold = roc_curve(ytest, test_score)
roc_auc = auc(fpr,tpr)
lw = 2
plt.figure(figsize=(5,5),dpi = 300)
plt.plot(fpr, tpr, color='burlywood',lw=lw,label='AUC = %0.3f'%roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.yticks(fontproperties = font3)
plt.xticks(fontproperties = font3)
plt.xlabel('False Positive Rate',font2)
plt.ylabel('True Positive Rate',font2)
plt.legend(loc="lower right",prop = font1)
plt.show()

#feature importance score
importances = bst.get_score(importance_type = "weight")
keys = list(importances.keys())
values = list(importances.values())
indices = np.argsort(values)[::-1]
height = []
name = []
for f in range(len(keys)):
    print("%s  %f" % (keys[indices[f]], values[indices[f]]))
    name.append(keys[indices[f]])
    height.append(values[indices[f]])
